## 区分Continuing Task和Episodic Task
episode即为从初始的状态到终止状态的整个过程
## 那么什么是Continuing Task？ 什么是Episodic Task呢？
两者最根本的区别就是Continuing Task是无法确认最终的终止状态，其动作状态会一直发生下去的即为Continuing Task
与此相反，Episodic Task是拥有有限长度的状态，也就是知道最终的结束状态

## 强化学习分类
强化学习基本上可以分为两大类。第一类是有模型的强化学习，另一类为无模型的强化学习。
模型可以简单地理解为状态转换概率transition-state probability。
动态规划(Dynamic Programming)就是一个基于有模型的强化学习方法。分类图如下所示：

$graph TD
强化学习-->有模型
有模型-->动态规划
强化学习-->无模型
无模型-->蒙特卡洛
无模型-->差分$sha


动态规划(Dynamic Programming)
根据前面推导出来的价值函数Value Functions的表达公式，我们可以看出当前状态的值可以通过接下来的状态值计算出来
但是，这两个状态的值函数我们都不知道。因此，该算法也被称为自举(bootstrapping)
接下来我们先看看动态规划方法如何对policy(策略)来进行评估的
策略评估(Policy Evaluation)
如何来评估策略的好坏呢？这时候我们需要从数学的角度出发，也就是要通过数值的方式来对策略进行评估。数值大代表策略好，数值小代表策略不是很好，我们很容易就会想到Value Functions。
根据贝尔曼方程Bellman Equation我们得到状态值state value的计算公式如下：
（贝尔曼方程（Bellman Equation）也被称作动态规划方程）


                        
于是，我们就可以通过一次次的迭代来得到该policy的state value了。其伪代码如下：

从伪代码中，我们可以知道Policy Evaluation是对一个Policy进行计算的。当我们更新的state value值几乎不变时，就可以让其停止迭代了。


例1: 我们有一个Gridworld的游戏，其中有2个终点，1-14数字分别代表14个不同的状态(state)，我们可以朝四个方向走(上下左右)，也就是代表了我们的动作(action)，每一次状态的改变都会得到-1的奖励。游戏如下图所示：




贝尔曼方程解释


回报和返还（return）
正如前面所讨论的，强化学习agent如何最大化累积未来的回报。用于描述累积未来回报的词是返还，通常用R表示。我们还使用一个下标t来表示某个时间步长的返还。在数学符号中，它是这样的:
相比使用未来的累积回报作为返还，更常见的是使用未来的累积折现回报（cumulative discounted reward）:

我们做决策是为了最大化未来的回报，需要先估算价值函数，
回报函数

其中衰减系数体现了未来的奖励在当前时刻的价值比例
γ接近0，则表明趋向于“近视”性评估（更注重眼前的价值）；γ接近1则表明偏重考虑远期的利益
那么Value Function该如何定义？
就是期望的回报，期望的回报越高，价值显然也就越大，也就越值得去选择。用数学来定义就是如下：
St=t 表示：价值函数从该状态开始收获的期望

约定用状态价值函数或价值函数来描述针对状态的价值；
用行为价值函数来描述某一状态下执行某一行为的价值

*最后一步中，

指的是下一状态的价值函数，$m^3$










